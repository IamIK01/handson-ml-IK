{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### 1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "**Answer:**\n",
    "- **Parameter Sharing**: CNNs use shared weights in convolutional layers, which means the same filter (or set of filters) is used across different parts of the input. This greatly reduces the number of parameters compared to fully connected layers.\n",
    "- **Sparsity of Connections**: In CNNs, each neuron is connected only to a local region of the input, reducing the number of connections and parameters.\n",
    "- **Translation Invariance**: CNNs can detect features regardless of their position in the image, thanks to the nature of convolution operations.\n",
    "- **Hierarchical Feature Learning**: CNNs automatically learn to detect low-level features (like edges) in the initial layers and high-level features (like objects) in deeper layers.\n",
    "\n",
    "#### 2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "**Answer:**\n",
    "\n",
    "1. **First Convolutional Layer**:\n",
    "   - Input: 200 x 300 x 3\n",
    "   - Stride: 2\n",
    "   - \"Same\" padding ensures the output size is \\( $\\lceil \\frac{\\text{input size}}{2} \\rceil$ \\)\n",
    "   - Output size (height, width): \\( $\\lceil \\frac{200}{2} \\rceil \\times \\lceil \\frac{300}{2} \\rceil$ \\) = 100 x 150\n",
    "   - Output: 100 x 150 x 100 (100 feature maps)\n",
    "   \n",
    "2. **Second Convolutional Layer**:\n",
    "   - Input: 100 x 150 x 100\n",
    "   - Stride: 2\n",
    "   - Output size (height, width): \\( $\\lceil \\frac{100}{2} \\rceil \\times \\lceil \\frac{150}{2} \\rceil$ \\) = 50 x 75\n",
    "   - Output: 50 x 75 x 200 (200 feature maps)\n",
    "   \n",
    "3. **Third Convolutional Layer**:\n",
    "   - Input: 50 x 75 x 200\n",
    "   - Stride: 2\n",
    "   - Output size (height, width): \\( $\\lceil \\frac{50}{2} \\rceil \\times \\lceil \\frac{75}{2} \\rceil$ \\) = 25 x 38\n",
    "   - Output: 25 x 38 x 400 (400 feature maps)\n",
    "\n",
    "##### Total Number of Parameters in the CNN:\n",
    "\n",
    "1. **First Convolutional Layer**:\n",
    "   - Number of filters: 100\n",
    "   - Filter size: \\(3 $\\times$ 3 $\\times$ 3\\)\n",
    "   - Parameters per filter: \\(3 $\\times$ 3 $\\times$ 3 = 27\\)\n",
    "   - Total parameters (including biases): \\(100 $\\times$ (27 + 1) = 2800\\)\n",
    "\n",
    "2. **Second Convolutional Layer**:\n",
    "   - Number of filters: 200\n",
    "   - Filter size: \\(3 $\\times$ 3 $\\times$ 100\\)\n",
    "   - Parameters per filter: \\(3 $\\times$ 3 $\\times$ 100 = 900\\)\n",
    "   - Total parameters (including biases): \\(200 $\\times$ (900 + 1) = 180200\\)\n",
    "\n",
    "3. **Third Convolutional Layer**:\n",
    "   - Number of filters: 400\n",
    "   - Filter size: \\(3 $\\times$ 3 $\\times$ 200\\)\n",
    "   - Parameters per filter: \\(3 $\\times$ 3 $\\times$ 200 = 1800\\)\n",
    "   - Total parameters (including biases): \\(400 $\\times$ (1800 + 1) = 720400\\)\n",
    "\n",
    "- **Total parameters in the CNN**: \\(2800 + 180200 + 720400 = 903400\\)\n",
    "\n",
    "##### RAM Requirement for a Single Instance:\n",
    "\n",
    "- **Input image size**: \\(200 $\\times$ 300 $\\times$ 3 = 180000\\)\n",
    "- **First layer output size**: \\(100 $\\times$ 150 $\\times$ 100 = 1500000\\)\n",
    "- **Second layer output size**: \\(50 $\\times$ 75 $\\times$ 200 = 750000\\)\n",
    "- **Third layer output size**: \\(25 $\\times$ 38 $\\times$ 400 = 380000\\)\n",
    "\n",
    "- **Total activations**: \\(180000 + 1500000 + 750000 + 380000 = 2810000\\)\n",
    "\n",
    "- **Total activations in bytes (32-bit float = 4 bytes)**: \\(2810000 $\\times$ 4 = 11240000\\) bytes = 11.24 MB\n",
    "\n",
    "- **Parameters in bytes**: \\(903400 $\\times$ 4 = 3613600\\) bytes = 3.61 MB\n",
    "\n",
    "- **Total RAM for a single instance**: \\(11.24 + 3.61 = 14.85\\) MB\n",
    "\n",
    "##### RAM Requirement for a Mini-Batch of 50 Images:\n",
    "\n",
    "- **Activations for 50 images**: \\(2810000 $\\times$ 50 = 140500000\\)\n",
    "- **Activations in bytes**: \\(140500000 $\\times$ 4 = 562000000\\) bytes = 562 MB\n",
    "- **Parameters remain the same**: 3.61 MB\n",
    "\n",
    "- **Total RAM for a mini-batch of 50 images**: \\(562 + 3.61 = 565.61\\) MB\n",
    "\n",
    "##### Summary:\n",
    "\n",
    "- **Total parameters in the CNN**: 903,400\n",
    "- **Total RAM for a single instance**: 14.85 MB\n",
    "- **Total RAM for a mini-batch of 50 images**: 565.61 MB\n",
    "\n",
    "\n",
    "#### 3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "**Answer:**\n",
    "1. **Reduce the Mini-Batch Size**: Decreasing the number of samples in each mini-batch reduces memory usage.\n",
    "2. **Use Model Checkpointing**: Save intermediate states and use gradient checkpointing to recompute parts of the model during backpropagation instead of storing all intermediate activations.\n",
    "3. **Reduce the Input Image Size**: Decreasing the resolution of input images lowers memory requirements.\n",
    "4. **Simplify the Model**: Reduce the number of layers or the number of filters per layer to decrease the model size.\n",
    "5. **Use Mixed Precision Training**: Train the model with mixed precision (using both 16-bit and 32-bit floating point numbers) to save memory.\n",
    "\n",
    "#### 4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "**Answer:**\n",
    "- **Dimensionality Reduction**: Max pooling reduces the spatial dimensions of the input, which decreases the computational load and memory usage.\n",
    "- **Feature Selection**: Max pooling helps retain the most prominent features while discarding less important ones, which can improve generalization.\n",
    "- **Translation Invariance**: Max pooling introduces a degree of translational invariance, helping the network recognize objects regardless of minor positional changes.\n",
    "\n",
    "#### 5. When would you want to add a local response normalization layer?\n",
    "**Answer:**\n",
    "- **Competitive Normalization**: Local response normalization (LRN) layers can help highlight significant features by normalizing the responses across neighboring neurons, making the network more sensitive to strong activations.\n",
    "- **Improving Generalization**: LRN can improve generalization by reducing overfitting, especially in early layers of the network.\n",
    "- **Used in Specific Architectures**: LRN layers were particularly popular in earlier architectures like AlexNet but are less commonly used in modern architectures.\n",
    "\n",
    "#### 6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "**Answer:**\n",
    "- **AlexNet**: \n",
    "  - Deeper network with more filters per layer.\n",
    "  - Use of ReLU activation functions instead of tanh or sigmoid.\n",
    "  - Overlapping max pooling.\n",
    "  - Dropout for regularization.\n",
    "  - Data augmentation to reduce overfitting.\n",
    "  - Use of GPUs for faster training.\n",
    "  \n",
    "- **GoogLeNet (Inception)**:\n",
    "  - Inception modules that allow multiple convolutions with different kernel sizes to run in parallel.\n",
    "  - Reduction in the number of parameters by using 1x1 convolutions.\n",
    "  - Deep network with 22 layers.\n",
    "\n",
    "- **ResNet**:\n",
    "  - Introduction of residual connections (skip connections) to alleviate the vanishing gradient problem and enable training of very deep networks.\n",
    "  - Identity mappings in skip connections to make the optimization easier.\n",
    "\n",
    "- **SENet**:\n",
    "  - Introduction of Squeeze-and-Excitation (SE) blocks that adaptively recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels.\n",
    "  \n",
    "- **Xception**:\n",
    "  - Extreme version of Inception, where Inception modules are replaced with depthwise separable convolutions.\n",
    "  - Efficient combination of depthwise and pointwise convolutions to reduce computational cost while maintaining performance.\n",
    "\n",
    "#### 7. What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "**Answer:**\n",
    "- **Fully Convolutional Network (FCN)**: A network composed entirely of convolutional layers, without any dense (fully connected) layers, typically used for tasks like semantic segmentation where spatial information must be preserved throughout the network.\n",
    "  \n",
    "- **Converting a Dense Layer to a Convolutional Layer**: \n",
    "  - A dense layer with \\(n\\) neurons can be replaced by a convolutional layer with \\(n\\) filters of size 1x1. This makes each output feature map correspond to a single neuron in the dense layer while maintaining the spatial dimensions of the input.\n",
    "\n",
    "#### 8. What is the main technical difficulty of semantic segmentation?\n",
    "**Answer:**\n",
    "- **Precise Localization**: Semantic segmentation requires precise pixel-level classification, which is challenging because it demands accurate spatial information throughout the network. Maintaining high-resolution features and combining contextual information effectively while preserving spatial details is technically difficult. This often requires a combination of downsampling for context and upsampling (with techniques like deconvolution or unpooling) to restore spatial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'tf_gpu (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/im-ik01/miniconda3/envs/tf_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from utils import set_seed\n",
    "import os\n",
    "\n",
    "set_seed()\n",
    "\n",
    "ROOT_DIR = './'\n",
    "datapath = os.path.join(ROOT_DIR, 'datasets')\n",
    "os.makedirs(datapath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the MNIST dataset using tfds\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train[:80%]', 'train[80%:]', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    data_dir=datapath\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
