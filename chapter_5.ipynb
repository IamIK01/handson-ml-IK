{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Fundamental Idea Behind SVMs\n",
    "Support Vector Machines aim to find the optimal separating hyperplane between two classes that maximizes the margin, i.e., the distance between the nearest data points of the classes (support vectors).\n",
    "\n",
    "### Q2: Support Vector Definition\n",
    "Support vectors are the data points that lie closest to the decision surface (or hyperplane) and are pivotal in defining the boundary. The SVM classifier is primarily influenced by these points.\n",
    "\n",
    "### Q3: Importance of Scaling Inputs in SVMs\n",
    "Input scaling in SVMs is crucial because the algorithm depends on calculating the distance between data points. Unscaled features can skew these distances and lead to a biased hyperplane that does not generalize well.\n",
    "\n",
    "### Q4: SVM Confidence Scores and Probabilities\n",
    "An SVM classifier typically provides a confidence score based on the data point's distance from the hyperplane. Probabilities can be derived from these scores using additional methods like Platt scaling.\n",
    "\n",
    "### Q5: Primal vs. Dual SVM Problem for Large Datasets\n",
    "For large training sets with fewer features, solving the primal problem is generally more efficient. The dual is preferable for kernelized SVMs or when the number of features is much greater than the number of instances.\n",
    "\n",
    "### Q6: Adjusting \\( $\\gamma$ \\) and \\( $C$ \\) in SVM with RBF Kernel for Underfitting\n",
    "To combat underfitting in an SVM with an RBF kernel, increasing \\( $\\gamma$ \\) makes the decision boundary more flexible, while increasing \\( $C$ \\) allows for a greater margin of error in classification, thus capturing more complexity.\n",
    "\n",
    "## 7. Soft Margin Linear SVM Classifier with QP Solver\n",
    "\n",
    "To set the QP parameters for solving a soft margin linear SVM classifier problem, configure:\n",
    "\n",
    "- `H`: This is a matrix where each element `H[i][j]` is the dot product of the ith and jth training instances multiplied by their respective labels. It defines the curvature of the quadratic optimization problem.\n",
    "- `f`: A vector that represents the linear part of the objective function. For SVM, this is typically set to a vector of -1s since we want to minimize the inverse of the distance of the margin.\n",
    "- `A`: The constraint matrix that enforces the class labels. In the context of SVM, it's a diagonal matrix where each entry `A[i][i]` is the label of the ith instance.\n",
    "- `b`: A vector of ones, which corresponds to the constraint that the slack variables must be greater than 1, scaled by the label of the instance.\n",
    "\n",
    "## 8. Comparing LinearSVC, SVC, and SGDClassifier\n",
    "\n",
    "- **LinearSVC**: Optimized for linear SVMs by using the liblinear library. It's suitable for large datasets and supports explicit feature mapping.\n",
    "- **SVC with a linear kernel**: Uses the libsvm library and supports the kernel trick. Typically slower on large datasets, but effective for datasets where the number of features is high compared to the number of samples.\n",
    "- **SGDClassifier**: Implements a linear classifier with stochastic gradient descent learning. It's very flexible and can handle large datasets efficiently.\n",
    "\n",
    "These classifiers can be compared on a linearly separable dataset to understand their performance and decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 0.80\n",
      "SVC accuracy: 0.80\n",
      "SGDClassifier accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ivanj\\.conda\\envs\\handson_ml2\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a linearly separable dataset\n",
    "X, y = make_classification(n_features=4, random_state=42, n_redundant=0, n_informative=4, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the LinearSVC\n",
    "linear_svc = LinearSVC(random_state=42)\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Train the SVC with linear kernel\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Train the SGDClassifier\n",
    "sgd_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Compare their performance\n",
    "models = {'LinearSVC': linear_svc, 'SVC': svc, 'SGDClassifier': sgd_clf}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'{name} accuracy: {accuracy_score(y_test, y_pred):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ivanj\\.conda\\envs\\handson_ml2\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy on MNIST: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Since SVMs are binary classifiers, we use one-versus-rest by default in SVC for multiclass classification\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Use a small validation set for hyperparameter tuning\n",
    "X_val_scaled, y_val = X_train_scaled[:10000], y_train[:10000]\n",
    "X_train_small, y_train_small = X_train_scaled[10000:], y_train[10000:]\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [1, 10], 'gamma': [0.001, 0.01]},\n",
    "]\n",
    "svm_clf = SVC()\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3)\n",
    "grid_search.fit(X_val_scaled, y_val)\n",
    "\n",
    "# Train the model with the best parameters found\n",
    "svm_clf = grid_search.best_estimator_\n",
    "svm_clf.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = svm_clf.predict(X_test_scaled)\n",
    "print(f'SVM accuracy on MNIST: {accuracy_score(y_test, y_pred):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM regressor MSE on California housing dataset: 0.42\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.01]},\n",
    "]\n",
    "svm_reg = SVR()\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train the model with the best parameters found\n",
    "svm_reg = grid_search.best_estimator_\n",
    "svm_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = svm_reg.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'SVM regressor MSE on California housing dataset: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handson_ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
