{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning and Random Forests Exercises\n",
    "\n",
    "1. **Combining Models with 95% Precision**\n",
    "   Yes, it is possible to combine multiple models to potentially achieve better results than any individual model through an ensemble method such as stacking. Stacking works by taking the predictions of each model and using them as input for a final predictor which makes the ultimate decision. This could improve precision if the errors of the individual models are uncorrelated.\n",
    "\n",
    "2. **Hard vs. Soft Voting Classifiers**\n",
    "   The difference between hard and soft voting classifiers is in how they aggregate the predictions of the individual learners:\n",
    "   - *Hard voting* predicts the final class based on the majority vote of the classifiers.\n",
    "   - *Soft voting* predicts the final class based on the weighted average probability of the class predicted by each classifier. This often achieves higher performance than hard voting because it gives more weight to highly confident votes.\n",
    "\n",
    "3. **Speeding Up Ensemble Training**\n",
    "   It is possible to speed up the training of a bagging ensemble by distributing it across multiple servers since each predictor in the ensemble is independent of the others. This is not the case with boosting ensembles, Random Forests, or stacking ensembles, as they typically need to train predictors sequentially, especially boosting which weights subsequent predictors based on the errors of the predecessors.\n",
    "\n",
    "4. **Benefits of Out-of-Bag Evaluation**\n",
    "   Out-of-bag (OOB) evaluation allows for an unbiased estimate of the ensemble predictor's performance without the need for a separate validation set. This is possible because in bagging, each predictor is trained on a different random subset of the training data, and the OOB samples are the unused instances which can serve as a test set.\n",
    "\n",
    "5. **What Makes Extra-Trees More Random than Random Forests**\n",
    "   Extra-Trees (Extremely Randomized Trees) introduce extra randomness compared to Regular Random Forests in the way splits are made. While Random Forests use a random subset of features to find the best possible thresholds, Extra-Trees make splits based on random thresholds for each feature rather than searching for the best possible thresholds. This extra randomness acts as a form of regularization and can help reduce variance while slightly increasing bias. Extra-Trees are generally faster to train because finding the best threshold for each feature at every split is one of the most time-consuming tasks in training Random Forests.\n",
    "\n",
    "6. **Hyperparameter Tweaking for AdaBoost Underfitting**\n",
    "   If an AdaBoost ensemble is underfitting the training data, you might want to:\n",
    "   - Increase the number of estimators, allowing the model to fit the training data more closely.\n",
    "   - Reduce the regularization hyperparameters of the base estimator, if applicable, to allow more complex models.\n",
    "   - Increase the learning rate to put more focus on correcting the errors of the preceding predictors.\n",
    "\n",
    "7. **Learning Rate Adjustment to Combat Overfitting**\n",
    "   When a Gradient Boosting ensemble overfits the training data, it is advisable to **decrease the learning rate**. This slows down the learning process and can lead to better generalization by requiring more weak learners to be combined to fit the training data, thus reducing the risk of fitting too closely to the training data noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(random_state=42)\n",
      "Training the ExtraTreesClassifier(random_state=42)\n",
      "Training the SVC(probability=True, random_state=42)\n",
      "RandomForestClassifier 0.9692\n",
      "ExtraTreesClassifier 0.9715\n",
      "SVC 0.9788\n",
      "Ensemble accuracy on validation set: 0.9791\n",
      "Ensemble accuracy on test set: 0.9767\n",
      "RandomForestClassifier accuracy on test set: 0.9645\n",
      "ExtraTreesClassifier accuracy on test set: 0.9691\n",
      "SVC accuracy on test set: 0.976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# 2. Split data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=10000, random_state=42)\n",
    "\n",
    "# 3. Train various classifiers\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma='scale', probability=True, random_state=42)\n",
    "\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate individual classifiers on the validation set\n",
    "for estimator in estimators:\n",
    "    y_pred = estimator.predict(X_val)\n",
    "    print(estimator.__class__.__name__, accuracy_score(y_val, y_pred))\n",
    "\n",
    "# 5. Combine them into an ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', random_forest_clf), ('et', extra_trees_clf), ('svc', svm_clf)],\n",
    "    voting='soft' # or 'hard' for hard voting\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate the ensemble on the validation set\n",
    "y_pred = voting_clf.predict(X_val)\n",
    "print(\"Ensemble accuracy on validation set:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# 7. Evaluate the ensemble on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Ensemble accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Comparison with individual classifiers\n",
    "for estimator in estimators:\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    print(estimator.__class__.__name__, \"accuracy on test set:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking ensemble accuracy on test set: 0.9648\n",
      "Voting classifier accuracy on test set: 0.9767\n",
      "The voting classifier outperforms the stacking ensemble.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a new training set for the blender\n",
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)\n",
    "\n",
    "# 2. Train the blender\n",
    "blender = LogisticRegression()\n",
    "blender.fit(X_val_predictions, y_val)\n",
    "\n",
    "# 3. Evaluate the ensemble on the test set\n",
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    " \n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)\n",
    "\n",
    "y_pred = blender.predict(X_test_predictions)\n",
    "print(\"Stacking ensemble accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Compare to the voting classifier's accuracy\n",
    "voting_clf_accuracy = accuracy_score(y_test, voting_clf.predict(X_test))\n",
    "print(\"Voting classifier accuracy on test set:\", voting_clf_accuracy)\n",
    "\n",
    "# Determine if the stacking ensemble outperforms the voting classifier\n",
    "if voting_clf_accuracy < accuracy_score(y_test, y_pred):\n",
    "    print(\"The stacking ensemble outperforms the voting classifier.\")\n",
    "else:\n",
    "    print(\"The voting classifier outperforms the stacking ensemble.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
